{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_global: int, movieId: int, imdbid: int, budget: int, original_title: string, popularity: string, revenue: string, runtime: string, vote_average: string, vote_count: string, gender_id: int, gender_name: string, tmdbId: int, userId: int, average_rating: double, cast_character: string, cast_namer: string, crew_job: string, crew_namer: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"movies_data_final\").getOrCreate()\n",
    "\n",
    "# Prepare the data\n",
    "df = spark.read.csv('../Data/Procceed/recommendations.csv', header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"average_rating\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the ratings for the test set\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1353.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 397.0 failed 1 times, most recent failure: Lost task 5.0 in stage 397.0 (TID 881) (192.168.56.1 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluate the model using the RegressionEvaluator\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m evaluator \u001b[39m=\u001b[39m RegressionEvaluator(metricName\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m\"\u001b[39m, labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m\"\u001b[39m, predictionCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rmse \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mevaluate(predictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRoot-mean-square error = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(rmse))\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(dataset)\n\u001b[0;32m    112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mevaluate(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1353.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 397.0 failed 1 times, most recent failure: Lost task 5.0 in stage 397.0 (TID 881) (192.168.56.1 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"average_rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the new movie's information\n",
    "new_movie = spark.createDataFrame([(0, 0, 10)], [\"userId\", \"movieId\", \"average_rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1125.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 289.0 failed 1 times, most recent failure: Lost task 14.0 in stage 289.0 (TID 610) (192.168.56.1 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(new_movie)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Extract the predicted rating\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predicted_rating \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mfirst()[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted rating for the new movie: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(predicted_rating))\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1938\u001b[0m, in \u001b[0;36mDataFrame.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Row]:\n\u001b[0;32m   1929\u001b[0m     \u001b[39m\"\"\"Returns the first row as a :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1930\u001b[0m \n\u001b[0;32m   1931\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[39m    Row(age=2, name='Alice')\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1938\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead()\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1924\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[39m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \n\u001b[0;32m   1899\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \u001b[39m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1924\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1925\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1926\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1924\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(\u001b[39m1\u001b[39m)\n\u001b[0;32m   1925\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1926\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:868\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(\u001b[39mself\u001b[39m, num: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Row]:\n\u001b[0;32m    859\u001b[0m     \u001b[39m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \n\u001b[0;32m    861\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlimit(num)\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \n\u001b[0;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1125.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 289.0 failed 1 times, most recent failure: Lost task 14.0 in stage 289.0 (TID 610) (192.168.56.1 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the rating for the new movie\n",
    "predictions = model.transform(new_movie)\n",
    "\n",
    "# Extract the predicted rating\n",
    "predicted_rating = predictions.select(\"prediction\").first()[0]\n",
    "print(\"Predicted rating for the new movie: \" + str(predicted_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m movie \u001b[39min\u001b[39;00m similar_movies:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mfilter(col(\u001b[39m\"\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m movie[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39moriginal_title\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m suggest_movies(\u001b[39m\"\u001b[39;49m\u001b[39mThe Shawshank Redemption\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 8\u001b[0m in \u001b[0;36msuggest_movies\u001b[1;34m(title, N)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest_movies\u001b[39m(title, N):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Get the movieId of the given movie title\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     movieId \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mfilter(col(\u001b[39m\"\u001b[39;49m\u001b[39moriginal_title\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m==\u001b[39;49m title)\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mmovieId\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcollect()[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Get the top N similar movies\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     similar_movies \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mitemFactors\u001b[39m.\u001b[39mfilter(col(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m movieId)\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Suggest top N movies similar to a given movie title\n",
    "def suggest_movies(title, N):\n",
    "    # Get the movieId of the given movie title\n",
    "    movieId = df.filter(col(\"original_title\") == title).select(\"movieId\").collect()[0][0]\n",
    "    \n",
    "    # Get the top N similar movies\n",
    "    similar_movies = model.itemFactors.filter(col(\"id\") == movieId).select(\"features\").collect()[0][0]\n",
    "    similar_movies = sorted(enumerate(similar_movies), key=lambda x: -x[1])[:N]\n",
    "    \n",
    "    # Print the similar movies\n",
    "    for movie in similar_movies:\n",
    "        print(df.filter(col(\"movieId\") == movie[0]).select(\"original_title\").collect()[0][0])\n",
    "\n",
    "suggest_movies(\"The Shawshank Redemption\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+--------------------+----------+-------+------------+----------+---------+-----------+------+------------------+\n",
      "|movieId|imdbid|  budget|id_global|      original_title|popularity|runtime|vote_average|vote_count|gender_id|gender_name|tmdbId|        avg_rating|\n",
      "+-------+------+--------+---------+--------------------+----------+-------+------------+----------+---------+-----------+------+------------------+\n",
      "|      1|114709|30000000|      862|           Toy Story| 21.946943|   81.0|         7.7|      5415|       16|  Animation|   862|3.8724696356275303|\n",
      "|      1|114709|30000000|      862|           Toy Story| 21.946943|   81.0|         7.7|      5415|       35|     Comedy|   862|3.8724696356275303|\n",
      "|      1|114709|30000000|      862|           Toy Story| 21.946943|   81.0|         7.7|      5415|    10751|     Family|   862|3.8724696356275303|\n",
      "|      2|113497|65000000|     8844|             Jumanji| 17.015539|  104.0|         6.9|      2413|       12|  Adventure|  8844|3.4018691588785046|\n",
      "|      2|113497|65000000|     8844|             Jumanji| 17.015539|  104.0|         6.9|      2413|       14|    Fantasy|  8844|3.4018691588785046|\n",
      "|      2|113497|65000000|     8844|             Jumanji| 17.015539|  104.0|         6.9|      2413|    10751|     Family|  8844|3.4018691588785046|\n",
      "|      3|113228|       0|    15602|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|    10749|    Romance| 15602|3.1610169491525424|\n",
      "|      3|113228|       0|    15602|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|       35|     Comedy| 15602|3.1610169491525424|\n",
      "|      4|114885|16000000|    31357|   Waiting to Exhale|      null|   null|        null|      null|       35|     Comedy| 31357|2.3846153846153846|\n",
      "|      4|114885|16000000|    31357|   Waiting to Exhale|      null|   null|        null|      null|       18|      Drama| 31357|2.3846153846153846|\n",
      "|      4|114885|16000000|    31357|   Waiting to Exhale|      null|   null|        null|      null|    10749|    Romance| 31357|2.3846153846153846|\n",
      "|      5|113041|       0|    11862|Father of the Bri...|  8.387519|  106.0|         5.7|       173|       35|     Comedy| 11862| 3.267857142857143|\n",
      "|      6|113277|60000000|      949|                Heat| 17.924927|  170.0|         7.7|      1886|       28|     Action|   949|3.8846153846153846|\n",
      "|      6|113277|60000000|      949|                Heat| 17.924927|  170.0|         7.7|      1886|       80|      Crime|   949|3.8846153846153846|\n",
      "|      6|113277|60000000|      949|                Heat| 17.924927|  170.0|         7.7|      1886|       18|      Drama|   949|3.8846153846153846|\n",
      "|      6|113277|60000000|      949|                Heat| 17.924927|  170.0|         7.7|      1886|       53|   Thriller|   949|3.8846153846153846|\n",
      "|      7|114319|58000000|    11860|             Sabrina|  6.677277|  127.0|         6.2|       141|       35|     Comedy| 11860|3.2830188679245285|\n",
      "|      7|114319|58000000|    11860|             Sabrina|  6.677277|  127.0|         6.2|       141|    10749|    Romance| 11860|3.2830188679245285|\n",
      "|      8|112302|       0|    45325|        Tom and Huck|  2.561161|   97.0|         5.4|        45|       28|     Action| 45325|               3.8|\n",
      "|      8|112302|       0|    45325|        Tom and Huck|  2.561161|   97.0|         5.4|        45|       12|  Adventure| 45325|               3.8|\n",
      "+-------+------+--------+---------+--------------------+----------+-------+------------+----------+---------+-----------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read csv\n",
    "df = spark.read.csv('../Data/Procceed/movies_data_final.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+\n",
      "|movieId|  budget|      original_title|popularity|runtime|vote_average|vote_count|gender_name|        avg_rating|\n",
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|  Animation|3.8724696356275303|\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|     Comedy|3.8724696356275303|\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|     Family|3.8724696356275303|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|  Adventure|3.4018691588785046|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|    Fantasy|3.4018691588785046|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|     Family|3.4018691588785046|\n",
      "|      3|       0|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|    Romance|3.1610169491525424|\n",
      "|      3|       0|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|     Comedy|3.1610169491525424|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|     Comedy|2.3846153846153846|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|      Drama|2.3846153846153846|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|    Romance|2.3846153846153846|\n",
      "|      5|       0|Father of the Bri...|  8.387519|  106.0|         5.7|       173|     Comedy| 3.267857142857143|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|     Action|3.8846153846153846|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|      Crime|3.8846153846153846|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|      Drama|3.8846153846153846|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|   Thriller|3.8846153846153846|\n",
      "|      7|58000000|             Sabrina|  6.677277|  127.0|         6.2|       141|     Comedy|3.2830188679245285|\n",
      "|      7|58000000|             Sabrina|  6.677277|  127.0|         6.2|       141|    Romance|3.2830188679245285|\n",
      "|      8|       0|        Tom and Huck|  2.561161|   97.0|         5.4|        45|     Action|               3.8|\n",
      "|      8|       0|        Tom and Huck|  2.561161|   97.0|         5.4|        45|  Adventure|               3.8|\n",
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete columns : gender_id, tmdbld, imdbid, id_global\n",
    "df = df.drop('gender_id', 'tmdbId', 'imdbid', 'id_global')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit,col\n",
    "\n",
    "# unique values in col gender_name\n",
    "df = df.withColumn(\"gender_name_Animation\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Action\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Adventure\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Comedy\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Crime\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Documentary\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Drama\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Family\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Fantasy\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Foreign\", lit(0))\n",
    "df = df.withColumn(\"gender_name_History\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Horreur\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Music\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Mystery\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Romance\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Science_Fiction\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Science_Thriller\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Science_TV_Movie\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Science_War\", lit(0))\n",
    "df = df.withColumn(\"gender_name_Science_Western\", lit(0))\n",
    "\n",
    "# Set the value in 'gender_name_female' column to 1 for the rows where 'gender_name' is \"Female\"\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Action\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Adventure\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Comedy\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Crime\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Documentary\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Drama\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "df = df.withColumn(\"gender_name_Animation\", when(col(\"gender_name\") == \"Animation\", 1).otherwise(col(\"gender_name_Animation\")))\n",
    "\n",
    "# Create a new column 'gender_name_male' with all values set to 0\n",
    "df = df.withColumn(\"gender_name_male\", lit(0))\n",
    "\n",
    "# Set the value in 'gender_name_male' column to 1 for the rows where 'gender_name' is \"Male\"\n",
    "df = df.withColumn(\"gender_name_male\", when(col(\"gender_name\") == \"Male\", 1).otherwise(col(\"gender_name_male\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+-----------------+\n",
      "|movieId|  budget|      original_title|popularity|runtime|vote_average|vote_count|gender_name|        avg_rating|gender_name_dummy|\n",
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+-----------------+\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|  Animation|3.8724696356275303|                0|\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|     Comedy|3.8724696356275303|                0|\n",
      "|      1|30000000|           Toy Story| 21.946943|   81.0|         7.7|      5415|     Family|3.8724696356275303|                0|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|  Adventure|3.4018691588785046|                0|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|    Fantasy|3.4018691588785046|                0|\n",
      "|      2|65000000|             Jumanji| 17.015539|  104.0|         6.9|      2413|     Family|3.4018691588785046|                0|\n",
      "|      3|       0|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|    Romance|3.1610169491525424|                0|\n",
      "|      3|       0|    Grumpier Old Men|   11.7129|  101.0|         6.5|        92|     Comedy|3.1610169491525424|                0|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|     Comedy|2.3846153846153846|                0|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|      Drama|2.3846153846153846|                0|\n",
      "|      4|16000000|   Waiting to Exhale|      null|   null|        null|      null|    Romance|2.3846153846153846|                0|\n",
      "|      5|       0|Father of the Bri...|  8.387519|  106.0|         5.7|       173|     Comedy| 3.267857142857143|                0|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|     Action|3.8846153846153846|                0|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|      Crime|3.8846153846153846|                0|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|      Drama|3.8846153846153846|                0|\n",
      "|      6|60000000|                Heat| 17.924927|  170.0|         7.7|      1886|   Thriller|3.8846153846153846|                0|\n",
      "|      7|58000000|             Sabrina|  6.677277|  127.0|         6.2|       141|     Comedy|3.2830188679245285|                0|\n",
      "|      7|58000000|             Sabrina|  6.677277|  127.0|         6.2|       141|    Romance|3.2830188679245285|                0|\n",
      "|      8|       0|        Tom and Huck|  2.561161|   97.0|         5.4|        45|     Action|               3.8|                0|\n",
      "|      8|       0|        Tom and Huck|  2.561161|   97.0|         5.4|        45|  Adventure|               3.8|                0|\n",
      "+-------+--------+--------------------+----------+-------+------------+----------+-----------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>budget</th>\n",
       "      <th>original_title</th>\n",
       "      <th>popularity</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>gender_name_Action</th>\n",
       "      <th>gender_name_Adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>gender_name_History</th>\n",
       "      <th>gender_name_Horror</th>\n",
       "      <th>gender_name_Music</th>\n",
       "      <th>gender_name_Mystery</th>\n",
       "      <th>gender_name_Romance</th>\n",
       "      <th>gender_name_Science Fiction</th>\n",
       "      <th>gender_name_TV Movie</th>\n",
       "      <th>gender_name_Thriller</th>\n",
       "      <th>gender_name_War</th>\n",
       "      <th>gender_name_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30000000</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>21.946943</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "      <td>3.872470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>65000000</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>17.015539</td>\n",
       "      <td>104.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "      <td>3.401869</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>11.712900</td>\n",
       "      <td>101.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.161017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>8.387519</td>\n",
       "      <td>106.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "      <td>3.267857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>60000000</td>\n",
       "      <td>Heat</td>\n",
       "      <td>17.924927</td>\n",
       "      <td>170.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>3.884615</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>161918</td>\n",
       "      <td>0</td>\n",
       "      <td>Sharknado 4: The 4th Awakens</td>\n",
       "      <td>4.574494</td>\n",
       "      <td>85.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8165</th>\n",
       "      <td>161944</td>\n",
       "      <td>8000000</td>\n",
       "      <td>The Last Brickmaker in America</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>85.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>162542</td>\n",
       "      <td>1000000</td>\n",
       "      <td>रुस्तम</td>\n",
       "      <td>7.333139</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167</th>\n",
       "      <td>162672</td>\n",
       "      <td>15050000</td>\n",
       "      <td>Mohenjo Daro</td>\n",
       "      <td>1.423358</td>\n",
       "      <td>155.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>163949</td>\n",
       "      <td>0</td>\n",
       "      <td>The Beatles: Eight Days a Week - The Touring Y...</td>\n",
       "      <td>7.078301</td>\n",
       "      <td>99.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8169 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId    budget                                     original_title  \\\n",
       "0           1  30000000                                          Toy Story   \n",
       "1           2  65000000                                            Jumanji   \n",
       "2           3         0                                   Grumpier Old Men   \n",
       "3           5         0                        Father of the Bride Part II   \n",
       "4           6  60000000                                               Heat   \n",
       "...       ...       ...                                                ...   \n",
       "8164   161918         0                       Sharknado 4: The 4th Awakens   \n",
       "8165   161944   8000000                     The Last Brickmaker in America   \n",
       "8166   162542   1000000                                             रुस्तम   \n",
       "8167   162672  15050000                                       Mohenjo Daro   \n",
       "8168   163949         0  The Beatles: Eight Days a Week - The Touring Y...   \n",
       "\n",
       "      popularity  runtime  vote_average  vote_count  avg_rating  \\\n",
       "0      21.946943     81.0           7.7      5415.0    3.872470   \n",
       "1      17.015539    104.0           6.9      2413.0    3.401869   \n",
       "2      11.712900    101.0           6.5        92.0    3.161017   \n",
       "3       8.387519    106.0           5.7       173.0    3.267857   \n",
       "4      17.924927    170.0           7.7      1886.0    3.884615   \n",
       "...          ...      ...           ...         ...         ...   \n",
       "8164    4.574494     85.0           4.3        88.0    1.500000   \n",
       "8165    0.038998     85.0           7.0         1.0    5.000000   \n",
       "8166    7.333139    150.0           7.3        25.0    5.000000   \n",
       "8167    1.423358    155.0           6.7        26.0    3.000000   \n",
       "8168    7.078301     99.0           7.6        92.0    5.000000   \n",
       "\n",
       "      gender_name_Action  gender_name_Adventure  ...  gender_name_History  \\\n",
       "0                      0                      0  ...                    0   \n",
       "1                      0                      1  ...                    0   \n",
       "2                      0                      0  ...                    0   \n",
       "3                      0                      0  ...                    0   \n",
       "4                      1                      0  ...                    0   \n",
       "...                  ...                    ...  ...                  ...   \n",
       "8164                   0                      0  ...                    0   \n",
       "8165                   0                      0  ...                    0   \n",
       "8166                   0                      0  ...                    0   \n",
       "8167                   0                      1  ...                    1   \n",
       "8168                   0                      0  ...                    0   \n",
       "\n",
       "      gender_name_Horror  gender_name_Music  gender_name_Mystery  \\\n",
       "0                      0                  0                    0   \n",
       "1                      0                  0                    0   \n",
       "2                      0                  0                    0   \n",
       "3                      0                  0                    0   \n",
       "4                      0                  0                    0   \n",
       "...                  ...                ...                  ...   \n",
       "8164                   1                  0                    0   \n",
       "8165                   0                  0                    0   \n",
       "8166                   0                  0                    0   \n",
       "8167                   0                  0                    0   \n",
       "8168                   0                  1                    0   \n",
       "\n",
       "      gender_name_Romance  gender_name_Science Fiction  gender_name_TV Movie  \\\n",
       "0                       0                            0                     0   \n",
       "1                       0                            0                     0   \n",
       "2                       1                            0                     0   \n",
       "3                       0                            0                     0   \n",
       "4                       0                            0                     0   \n",
       "...                   ...                          ...                   ...   \n",
       "8164                    0                            1                     0   \n",
       "8165                    0                            0                     0   \n",
       "8166                    1                            0                     0   \n",
       "8167                    1                            0                     0   \n",
       "8168                    0                            0                     0   \n",
       "\n",
       "      gender_name_Thriller  gender_name_War  gender_name_Western  \n",
       "0                        0                0                    0  \n",
       "1                        0                0                    0  \n",
       "2                        0                0                    0  \n",
       "3                        0                0                    0  \n",
       "4                        1                0                    0  \n",
       "...                    ...              ...                  ...  \n",
       "8164                     0                0                    0  \n",
       "8165                     0                0                    0  \n",
       "8166                     1                0                    0  \n",
       "8167                     0                0                    0  \n",
       "8168                     0                0                    0  \n",
       "\n",
       "[8169 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummies gender_name\n",
    "df = pd.get_dummies(df, columns=['gender_name'])\n",
    "\n",
    "# Group by movieId and sum only gender_name and reset index\n",
    "df = df.groupby(['movieId', 'budget', 'original_title', 'popularity', 'runtime', 'vote_average','vote_count', 'avg_rating']).sum().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>original_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Heat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId               original_title\n",
       "0        1                    Toy Story\n",
       "1        2                      Jumanji\n",
       "2        3             Grumpier Old Men\n",
       "3        5  Father of the Bride Part II\n",
       "4        6                         Heat"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save moviesId and original_title in a dataframe\n",
    "movies = df[['movieId', 'original_title']]\n",
    "movies.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggest top N movies similar to a given movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['movieId', 'original_title'])\n",
    "y= df['original_title']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'n_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m algo \u001b[39m=\u001b[39m KNNWithMeans(k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, sim_options\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_based\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m})\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m algo\u001b[39m.\u001b[39;49mfit(X)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\knns.py:175\u001b[0m, in \u001b[0;36mKNNWithMeans.fit\u001b[1;34m(self, trainset)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, trainset):\n\u001b[1;32m--> 175\u001b[0m     SymmetricAlgo\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m, trainset)\n\u001b[0;32m    176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_similarities()\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_x)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\knns.py:40\u001b[0m, in \u001b[0;36mSymmetricAlgo.fit\u001b[1;34m(self, trainset)\u001b[0m\n\u001b[0;32m     37\u001b[0m AlgoBase\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m, trainset)\n\u001b[0;32m     39\u001b[0m ub \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim_options[\u001b[39m\"\u001b[39m\u001b[39muser_based\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset\u001b[39m.\u001b[39mn_users \u001b[39mif\u001b[39;00m ub \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainset\u001b[39m.\u001b[39;49mn_items\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset\u001b[39m.\u001b[39mn_items \u001b[39mif\u001b[39;00m ub \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset\u001b[39m.\u001b[39mn_users\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset\u001b[39m.\u001b[39mur \u001b[39mif\u001b[39;00m ub \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainset\u001b[39m.\u001b[39mir\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'n_items'"
     ]
    }
   ],
   "source": [
    "algo = KNNWithMeans(k=10, sim_options={'name': 'cosine', 'user_based': False})\n",
    "algo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 predictions for each movie\n",
    "predictions = algo.get_neighbors(X, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Toy Story'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Compute the cosine similarity matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cosine_sim \u001b[39m=\u001b[39m cosine_similarity(df, df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cosine_sim\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Create a Series for the movie titles so they are associated to an ordered numerical list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1251\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \n\u001b[0;32m   1219\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[39mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1253\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1254\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    147\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    148\u001b[0m         X,\n\u001b[0;32m    149\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    157\u001b[0m         X,\n\u001b[0;32m    158\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    159\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    160\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    161\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    162\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         Y,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Toy Story'"
     ]
    }
   ],
   "source": [
    "# # del col : movieId, original_title\n",
    "# df = df.drop(columns=['movieId', 'original_title'])\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n",
    "# Create a Series for the movie titles so they are associated to an ordered numerical list\n",
    "indices = pd.Series(movies['original_title'])\n",
    "indices[:5]\n",
    "\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[indices == title].index[0]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return movies['original_title'].iloc[movie_indices]\n",
    "\n",
    "# Get the top 10 recommendations for the movie 'The Dark Knight Rises'\n",
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Toy Story'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\notebook\\4.0-SuggestNTopMovies.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Compute the cosine similarity matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cosine_sim \u001b[39m=\u001b[39m cosine_similarity(df, df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/notebook/4.0-SuggestNTopMovies.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cosine_sim\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1251\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \n\u001b[0;32m   1219\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[39mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1253\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1254\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    147\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    148\u001b[0m         X,\n\u001b[0;32m    149\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    157\u001b[0m         X,\n\u001b[0;32m    158\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    159\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    160\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    161\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    162\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         Y,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Toy Story'"
     ]
    }
   ],
   "source": [
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(df, df)\n",
    "cosine_sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c6a328c3dae7a0c12012862982ee9605db1881d32a9f60b075ff1e1dc4cc909"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
