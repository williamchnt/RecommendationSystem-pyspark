{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditsUrl = \"https://chenutfamily.freeboxos.fr:45883/share/z5zYVK5G0HSU5Elt/credits.csv\"\n",
    "keywordsUrl = \"https://chenutfamily.freeboxos.fr:45883/share/w7VuOdc5yL7_OooW/keywords.csv\" \n",
    "linksUrl = \"https://chenutfamily.freeboxos.fr:45883/share/Gmz7QCOWH6H0GqsX/links.csv\"\n",
    "linksSmallUrl = \"https://chenutfamily.freeboxos.fr:45883/share/63uzaFuqxgU0758p/links_small.csv\"\n",
    "moviesMetadataUrl = \"https://chenutfamily.freeboxos.fr:45883/share/vA7JOk9UHZRZxIwK/movies_metadata.csv\"\n",
    "ratingsUrl = \"https://chenutfamily.freeboxos.fr:45883/share/Q6XXQ-ScvAXlsjeY/ratings.csv\"\n",
    "ratingsSmallUrl = \"https://chenutfamily.freeboxos.fr:45883/share/ZZxgfteOX4i2C0Q4/ratings_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaDataPandas = pd.read_csv(moviesMetadataUrl,na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nostr\\OneDrive\\Documents\\GitHub\\RecommendationSystem\\1.0-Exploration.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/1.0-Exploration.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m linksSmall \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(pd\u001b[39m.\u001b[39mread_csv(linksSmallUrl))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/1.0-Exploration.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m moviesMetadata \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(metaDataPandas)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/1.0-Exploration.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ratings \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(pd\u001b[39m.\u001b[39;49mread_csv(ratingsUrl))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nostr/OneDrive/Documents/GitHub/RecommendationSystem/1.0-Exploration.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ratingsSmall \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(pd\u001b[39m.\u001b[39mread_csv(ratingsSmallUrl))\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[0;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    632\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    515\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[0;32m    516\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m--> 517\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[0;32m    518\u001b[0m     _merge_type,\n\u001b[0;32m    519\u001b[0m     (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data),\n\u001b[0;32m    520\u001b[0m )\n\u001b[0;32m    521\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[1;32m-> 1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[0;32m   1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()), name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname))\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[0;32m   1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[1;32m-> 1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()), name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\nostr\\anaconda3\\lib\\site-packages\\pyspark\\sql\\types.py:1365\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mnew_msg\u001b[39m(msg: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m   1363\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (name, msg)\n\u001b[1;32m-> 1365\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mnew_name\u001b[39m(n: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m   1366\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfield \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n, name)\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, NullType):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "credits = spark.createDataFrame(pd.read_csv(creditsUrl))\n",
    "keywords = spark.createDataFrame(pd.read_csv(keywordsUrl))\n",
    "links = spark.createDataFrame(pd.read_csv(linksUrl))\n",
    "linksSmall = spark.createDataFrame(pd.read_csv(linksSmallUrl))\n",
    "moviesMetadata = spark.createDataFrame(metaDataPandas)\n",
    "ratings = spark.createDataFrame(pd.read_csv(ratingsUrl))\n",
    "ratingsSmall = spark.createDataFrame(pd.read_csv(ratingsSmallUrl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      " |-- crew: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c6a328c3dae7a0c12012862982ee9605db1881d32a9f60b075ff1e1dc4cc909"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
